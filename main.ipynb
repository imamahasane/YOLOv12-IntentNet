{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, xml.etree.ElementTree as ET\n",
    "import pandas as pd, numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Helpers\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    try:\n",
    "        torch.backends.mps.manual_seed_all(seed)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    inter = max(0, xB-xA) * max(0, yB-yA)\n",
    "    A = (box1[2]-box1[0])*(box1[3]-box1[1])\n",
    "    B = (box2[2]-box2[0])*(box2[3]-box2[1])\n",
    "    return inter / (A + B - inter + 1e-9)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Config\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "config = {\n",
    "    \"dataset_root\": \"./PIE Dataset\",   # <-- your top‐level folder\n",
    "    \"seq_len\": 8,\n",
    "    \"img_size\": 320,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 30,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"num_workers\": 0,\n",
    "    \"lambda_cls\": 1.0,\n",
    "    \"lambda_reg\": 1.0,\n",
    "    \"iou_threshold_matching\": 0.5,\n",
    "    \"save_dir\": \"./checkpoints\",\n",
    "    \"yolo_weights\": \"yolov8n.pt\",\n",
    "}\n",
    "os.makedirs(config[\"save_dir\"], exist_ok=True)\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "config[\"device\"] = device\n",
    "\n",
    "print(\"Using device:\", config[\"device\"])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Dataset\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class PIESequenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, seq_len=8, img_size=224):\n",
    "        self.root = root_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.img_size = img_size\n",
    "\n",
    "        ann_root = os.path.join(root_dir, \"annotations\")\n",
    "        records = []\n",
    "        for set_name in sorted(os.listdir(ann_root)):\n",
    "            ann_set = os.path.join(ann_root, set_name)\n",
    "            vid_set = os.path.join(root_dir, set_name)\n",
    "            if not os.path.isdir(vid_set):\n",
    "                continue\n",
    "\n",
    "            for xml_file in sorted(os.listdir(ann_set)):\n",
    "                if not xml_file.endswith(\".xml\"):\n",
    "                    continue\n",
    "                video_id = xml_file.replace(\"_annt.xml\", \"\")\n",
    "                xml_path = os.path.join(ann_set, xml_file)\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                for track in root.findall(\"track\"):\n",
    "                    track_id = int(track.attrib.get(\"id\", 0))\n",
    "                    for box in track.findall(\"box\"):\n",
    "                        frame_id = int(box.attrib[\"frame\"])\n",
    "                        x1 = float(box.attrib[\"xtl\"])\n",
    "                        y1 = float(box.attrib[\"ytl\"])\n",
    "                        x2 = float(box.attrib[\"xbr\"])\n",
    "                        y2 = float(box.attrib[\"ybr\"])\n",
    "\n",
    "                        cl_lab, ftc = 0, -1\n",
    "                        for attr in box.findall(\"attribute\"):\n",
    "                            name = attr.attrib.get(\"name\", \"\")\n",
    "                            if name == \"crossing_label\":\n",
    "                                cl_lab = int(attr.text)\n",
    "                            elif name == \"frames_to_cross\":\n",
    "                                ftc = int(attr.text)\n",
    "\n",
    "                        records.append({\n",
    "                            \"set\": set_name,\n",
    "                            \"video\": video_id,\n",
    "                            \"frame_id\": frame_id,\n",
    "                            \"track_id\": track_id,\n",
    "                            \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2,\n",
    "                            \"crossing_label\": cl_lab,\n",
    "                            \"frames_to_cross\": ftc\n",
    "                        })\n",
    "\n",
    "        self.df = pd.DataFrame.from_records(records)\n",
    "\n",
    "        # 2) Build sliding‐window sequences\n",
    "        self.sequences = []\n",
    "        grouped = self.df.groupby([\"set\", \"video\", \"track_id\"])\n",
    "        for (set_name, video_id, track_id), g in grouped:\n",
    "            g = g.sort_values(\"frame_id\")\n",
    "            fids = g.frame_id.tolist()\n",
    "            bbs = list(zip(g.x1, g.y1, g.x2, g.y2))\n",
    "            cll = g.crossing_label.tolist()\n",
    "            ftc = g.frames_to_cross.tolist()\n",
    "\n",
    "            for i in range(len(fids) - seq_len + 1):\n",
    "                sf = fids[i : i+seq_len]\n",
    "                sb = bbs[i : i+seq_len]\n",
    "                lbl_cls = int(cll[i+seq_len-1] == 1)\n",
    "                lbl_reg = ftc[i+seq_len-1] if ftc[i+seq_len-1] >= 0 else -1\n",
    "                self.sequences.append({\n",
    "                    \"set\": set_name,\n",
    "                    \"video\": video_id,\n",
    "                    \"frame_ids\": sf,\n",
    "                    \"bboxes\": sb,\n",
    "                    \"label_cls\": lbl_cls,\n",
    "                    \"label_reg\": lbl_reg\n",
    "                })\n",
    "\n",
    "        print(f\"Total sequences: {len(self.sequences)}\")\n",
    "\n",
    "        self.frame_tf = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.video_cache = {}\n",
    "\n",
    "    def load_video(self, set_name, video_id):\n",
    "        key = (set_name, video_id)\n",
    "        if key in self.video_cache:\n",
    "            return self.video_cache[key]\n",
    "\n",
    "        video_path = os.path.join(self.root, set_name, f\"{video_id}.mp4\")\n",
    "        v, _, _ = read_video(video_path, pts_unit=\"sec\")  # [T, H, W, C]\n",
    "        v = v.permute(0, 3, 1, 2)                           # [T, C, H0, W0]\n",
    "        H0, W0 = v.shape[2], v.shape[3]\n",
    "        self.video_cache[key] = (v, (H0, W0))\n",
    "        return self.video_cache[key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        v, (H0, W0) = self.load_video(seq[\"set\"], seq[\"video\"])\n",
    "        scale_x, scale_y = 640 / W0, 640 / H0\n",
    "\n",
    "        frames = []\n",
    "        scaled_bbs = []\n",
    "        for fid, (x1, y1, x2, y2) in zip(seq[\"frame_ids\"], seq[\"bboxes\"]):\n",
    "            frm = v[fid]                    # (C, H0, W0)\n",
    "            tfrm = self.frame_tf(frm)       # → (3, 640, 640)\n",
    "            frames.append(tfrm)\n",
    "\n",
    "            scaled_bbs.append([\n",
    "                x1 * scale_x, y1 * scale_y,\n",
    "                x2 * scale_x, y2 * scale_y\n",
    "            ])\n",
    "\n",
    "        frames = torch.stack(frames, dim=0)                    # (S, 3, 640, 640)\n",
    "        bboxes = torch.tensor(scaled_bbs, dtype=torch.float32) # (S, 4)\n",
    "        return (\n",
    "            frames,\n",
    "            bboxes,\n",
    "            torch.tensor(seq[\"label_cls\"], dtype=torch.float32),\n",
    "            torch.tensor(seq[\"label_reg\"], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Model definition\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class YOLOv12Backbone(nn.Module):\n",
    "    def __init__(self, weights_path=\"yolov8n.pt\", iou_thresh=0.5, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model = YOLO(weights_path).to(device)\n",
    "        self.iou_thresh = iou_thresh\n",
    "\n",
    "        self.fc = nn.Linear(5, 64)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, frames, gt_bboxes):\n",
    "        B, S, C, H, W = frames.shape\n",
    "        out = torch.zeros(B, S, 64, device=self.device)\n",
    "\n",
    "        for b in range(B):\n",
    "            for t in range(S):\n",
    "\n",
    "                single = frames[b, t].unsqueeze(0).to(self.device)\n",
    "                results = self.model.predict(\n",
    "                    source=single,\n",
    "                    imgsz=H,\n",
    "                    device=self.device,\n",
    "                    save=False,\n",
    "                    save_txt=False\n",
    "                )[0].boxes\n",
    "\n",
    "                best_iou  = 0.0\n",
    "                best_feat = torch.zeros(5, device=self.device)\n",
    "\n",
    "                if results.shape[0] > 0:\n",
    "                    boxes = results.xyxy.cpu().numpy()   # (N,4)\n",
    "                    confs = results.conf.cpu().numpy()   # (N,)\n",
    "                    for i, box in enumerate(boxes):\n",
    "                        iou = bbox_iou(box.tolist(), gt_bboxes[b, t].tolist())\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_feat = torch.tensor(\n",
    "                                [box[0], box[1], box[2], box[3], float(confs[i])],\n",
    "                                device=self.device\n",
    "                            )\n",
    "\n",
    "                if best_iou < self.iou_thresh:\n",
    "                    best_feat = torch.zeros(5, device=self.device)\n",
    "\n",
    "                out[b, t] = self.fc(best_feat)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SimpleCNNBackbone(nn.Module):\n",
    "    def __init__(self, img_size=224):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        conv_out = (img_size//4)*(img_size//4)*64\n",
    "        self.fc    = nn.Linear(conv_out, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.tr = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, D = x.size()\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        out = self.tr(torch.cat([cls, x], dim=1))\n",
    "        return out[:, 0]\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.intent_fc = nn.Linear(embed_dim, 1)\n",
    "        self.time_fc   = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intent = torch.sigmoid(self.intent_fc(x)).squeeze(-1)\n",
    "        ttc    = self.time_fc(x).squeeze(-1)\n",
    "        return intent, ttc\n",
    "\n",
    "class YOLOv12_IntentNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.device = cfg[\"device\"]\n",
    "        self.yolo_backbone = YOLOv12Backbone(\n",
    "            cfg[\"yolo_weights\"], cfg[\"iou_threshold_matching\"], cfg[\"device\"]\n",
    "        )\n",
    "        self.cnn_backbone  = SimpleCNNBackbone(cfg[\"img_size\"])\n",
    "        self.linear_fuse   = nn.Linear(64 + 128, 256)\n",
    "        self.temporal_model= TemporalTransformer(256, 4, 2)\n",
    "        self.pred_head     = PredictionHead(256)\n",
    "\n",
    "    def forward(self, frames, gt_bboxes):\n",
    "        B, S, C, H, W = frames.shape\n",
    "        y_feats = self.yolo_backbone(frames, gt_bboxes)  # (B, S, 64)\n",
    "\n",
    "        patches = []\n",
    "        for b in range(B):\n",
    "            for t in range(S):\n",
    "                arr = (frames[b,t].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "                pil = Image.fromarray(arr)\n",
    "                x1,y1,x2,y2 = gt_bboxes[b,t].cpu().int().tolist()\n",
    "                if x2<=x1 or y2<=y1:\n",
    "\n",
    "                    cx, cy = W//2, H//2\n",
    "                    x1_,y1_ = cx-self.img_size//2, cy-self.img_size//2\n",
    "                    x2_,y2_ = x1_+self.img_size, y1_+self.img_size\n",
    "                    patch = pil.crop((x1_,y1_,x2_,y2_))\n",
    "                else:\n",
    "                    patch = pil.crop((x1,y1,x2,y2))\n",
    "                patch = patch.resize((self.img_size, self.img_size))\n",
    "                patch_t = transforms.ToTensor()(patch)\n",
    "                patch_t = transforms.Normalize(\n",
    "                    mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]\n",
    "                )(patch_t)\n",
    "                patches.append(patch_t)\n",
    "        patches = torch.stack(patches, 0).to(self.device)\n",
    "        cnn_feats = self.cnn_backbone(patches).view(B, S, 128)\n",
    "\n",
    "        fus = self.linear_fuse(torch.cat([y_feats, cnn_feats], dim=2))\n",
    "        cls_tok = self.temporal_model(fus)\n",
    "        return self.pred_head(cls_tok)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_cls=1.0, lambda_reg=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.l1  = nn.SmoothL1Loss()\n",
    "        self.lc, self.lr = lambda_cls, lambda_reg\n",
    "\n",
    "    def forward(self, intent_p, ttc_p, label_cls, label_reg):\n",
    "        loss_c = self.bce(intent_p, label_cls)\n",
    "        mask   = (label_reg >= 0)\n",
    "        if mask.sum()>0:\n",
    "            loss_r = self.l1(ttc_p[mask], label_reg[mask])\n",
    "        else:\n",
    "            loss_r = torch.tensor(0.0, device=intent_p.device)\n",
    "        return self.lc*loss_c + self.lr*loss_r, loss_c.detach(), loss_r.detach()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Training / Validation\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def train_one_epoch(model, loader, opt, crit, device):\n",
    "    model.train()\n",
    "    for i, (fr, bb, lc, lr) in enumerate(loader, 1):\n",
    "        fr, bb, lc, lr = fr.to(device), bb.to(device), lc.to(device), lr.to(device)\n",
    "        opt.zero_grad()\n",
    "        pi, pt = model(fr, bb)\n",
    "        loss, _, _ = crit(pi, pt, lc, lr)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % 20 == 0:\n",
    "            print(f\"  Batch {i}/{len(loader)} | Loss {loss.item():.4f}\")\n",
    "\n",
    "def validate_one_epoch(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    losses, preds, labs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for fr, bb, lc, lr in loader:\n",
    "            fr, bb, lc, lr = fr.to(device), bb.to(device), lc.to(device), lr.to(device)\n",
    "            pi, pt = model(fr, bb)\n",
    "            losses.append(crit(pi, pt, lc, lr)[1].item())\n",
    "            preds.append((pi>=0.5).float().cpu())\n",
    "            labs.append(lc.cpu())\n",
    "    acc = (torch.cat(preds)==torch.cat(labs)).float().mean().item()\n",
    "    return np.mean(losses), acc\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # Dataset & Split\n",
    "    ds = PIESequenceDataset(config[\"dataset_root\"], config[\"seq_len\"], config[\"img_size\"])\n",
    "    idx = list(range(len(ds))); random.shuffle(idx)\n",
    "    split = int(0.8 * len(idx))\n",
    "    train_ds = torch.utils.data.Subset(ds, idx[:split])\n",
    "    val_ds   = torch.utils.data.Subset(ds, idx[split:])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                              num_workers=config[\"num_workers\"], pin_memory=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                              num_workers=config[\"num_workers\"], pin_memory=False)\n",
    "\n",
    "    model = YOLOv12_IntentNet(config).to(config[\"device\"])\n",
    "    opt   = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-4)\n",
    "    crit  = CombinedLoss(config[\"lambda_cls\"], config[\"lambda_reg\"])\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    for ep in range(1, config[\"num_epochs\"]+1):\n",
    "        print(f\"\\n===== Epoch {ep}/{config['num_epochs']} =====\")\n",
    "        t0 = time.time()\n",
    "        train_one_epoch(model, train_loader, opt, crit, config[\"device\"])\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, crit, config[\"device\"])\n",
    "        print(f\"Validation → Loss: {val_loss:.4f} | Acc: {val_acc*100:.2f}% | Time: {(time.time()-t0):.0f}s\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            ckpt = os.path.join(config[\"save_dir\"], f\"best_epoch{ep}.pt\")\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            print(\"  → Saved checkpoint:\", ckpt)\n",
    "            \n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
