{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# For computing IoU\n",
    "def bbox_iou(box1, box2):\n",
    "    # Intersection\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    inter_w = max(0, xB - xA)\n",
    "    inter_h = max(0, yB - yA)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    # Areas\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area + 1e-9\n",
    "\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # For MPS compatibility:\n",
    "    try:\n",
    "        torch.backends.mps.manual_seed_all(seed)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"dataset_root\": \"pie_dataset\",\n",
    "    \"seq_len\": 8,              # number of frames in each sequence\n",
    "    \"img_size\": 224,           # crop size for each pedestrian patch\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 30,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"num_workers\": 2,\n",
    "    \"lambda_cls\": 1.0,\n",
    "    \"lambda_reg\": 1.0,\n",
    "    \"lambda_det\": 1.0,\n",
    "    # Where to save model checkpoints\n",
    "    \"save_dir\": \"./checkpoints\",\n",
    "    # YOLO weights (placeholder for YOLOv12). \n",
    "    \"yolo_weights\": \"yolov8n.pt\",\n",
    "    \"iou_threshold_matching\": 0.5,\n",
    "}\n",
    "\n",
    "os.makedirs(config[\"save_dir\"], exist_ok=True)\n",
    "print(\"Using device:\", config[\"device\"])\n",
    "\n",
    "\n",
    "class PIESequenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, seq_len=8, img_size=224, transform=None):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # 1. Load annotations CSV\n",
    "        ann_path = os.path.join(root_dir, \"annotations.csv\")\n",
    "        if not os.path.exists(ann_path):\n",
    "            raise FileNotFoundError(f\"Cannot find {ann_path}. Make sure annotations.csv exists.\")\n",
    "\n",
    "        self.df = pd.read_csv(ann_path)\n",
    "        # Ensure required columns exist\n",
    "        required_cols = {\n",
    "            \"video\", \"frame_id\", \"track_id\",\n",
    "            \"x1\", \"y1\", \"x2\", \"y2\",\n",
    "            \"crossing_label\", \"frames_to_cross\"\n",
    "        }\n",
    "        if not required_cols.issubset(set(self.df.columns)):\n",
    "            raise ValueError(f\"annotations.csv must contain columns: {required_cols}\")\n",
    "\n",
    "        # 2. Group by (video, track_id) and sort by frame_id\n",
    "        grouped = self.df.groupby([\"video\", \"track_id\"])\n",
    "        self.sequences = []\n",
    "        for (video, track_id), group in grouped:\n",
    "            group_sorted = group.sort_values(\"frame_id\")\n",
    "            frame_ids = group_sorted[\"frame_id\"].tolist()\n",
    "            bboxes = list(zip(group_sorted[\"x1\"], group_sorted[\"y1\"],\n",
    "                              group_sorted[\"x2\"], group_sorted[\"y2\"]))\n",
    "            crossing_labels = group_sorted[\"crossing_label\"].tolist()\n",
    "            frames_to_cross = group_sorted[\"frames_to_cross\"].tolist()\n",
    "\n",
    "            # Sliding window\n",
    "            num_frames = len(frame_ids)\n",
    "            for start_idx in range(num_frames - seq_len + 1):\n",
    "                end_idx = start_idx + seq_len\n",
    "                seq_frame_ids = frame_ids[start_idx:end_idx]\n",
    "                seq_bboxes = bboxes[start_idx:end_idx]\n",
    "                seq_crossing_labels = crossing_labels[start_idx:end_idx]\n",
    "                seq_frames_to_cross = frames_to_cross[start_idx:end_idx]\n",
    "\n",
    "                # We define Y_cls = 1 if the *last* frame in this seq has crossing_label==1\n",
    "                label_cls = int(seq_crossing_labels[-1] == 1)\n",
    "                # For regression: take frames_to_cross of the *last* frame\n",
    "                ftc = seq_frames_to_cross[-1]\n",
    "                if ftc >= 0:\n",
    "                    label_reg = ftc  # We will convert to seconds in the model\n",
    "                else:\n",
    "                    label_reg = -1   # Means “no crossing soon”\n",
    "\n",
    "                self.sequences.append({\n",
    "                    \"video\": video,\n",
    "                    \"frame_ids\": seq_frame_ids,\n",
    "                    \"bboxes\": seq_bboxes,\n",
    "                    \"label_cls\": label_cls,\n",
    "                    \"label_reg\": label_reg\n",
    "                })\n",
    "\n",
    "        print(f\"Total sequences: {len(self.sequences)}\")\n",
    "\n",
    "        # Define transforms for the cropped patch (after detection matching):\n",
    "        self.patch_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Transform to feed full frame into YOLO: only resize so YOLO runs faster\n",
    "        self.frame_transform = transforms.Compose([\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_info = self.sequences[idx]\n",
    "        video = seq_info[\"video\"]\n",
    "        frame_ids = seq_info[\"frame_ids\"]\n",
    "        bboxes = seq_info[\"bboxes\"]\n",
    "        label_cls = seq_info[\"label_cls\"]\n",
    "        label_reg = seq_info[\"label_reg\"]\n",
    "\n",
    "        full_frames = []\n",
    "        for i, fid in enumerate(frame_ids):\n",
    "            img_path = os.path.join(self.root_dir, \"images\", video, f\"{fid}.jpg\")\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Missing image: {img_path}\")\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_t = self.frame_transform(img)  # for YOLO\n",
    "            full_frames.append(img_t)\n",
    "\n",
    "        frames_tensor = torch.stack(full_frames, dim=0)  # (seq_len, 3, 640, 640)\n",
    "\n",
    "        # Return bboxes in a Tensor as well\n",
    "        bboxes_tensor = torch.tensor(bboxes, dtype=torch.float32)  # (seq_len, 4)\n",
    "\n",
    "        return frames_tensor, bboxes_tensor, torch.tensor(label_cls, dtype=torch.float32), torch.tensor(label_reg, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class YOLOv12Backbone(nn.Module):\n",
    "    def __init__(self, weights_path=\"yolov8n.pt\", iou_thresh=0.5, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Load Ultralytics YOLO model\n",
    "        self.model = YOLO(weights_path)\n",
    "        self.model.to(device)\n",
    "        self.iou_thresh = iou_thresh\n",
    "\n",
    "        # A small MLP to turn (x1,y1,x2,y2,conf) → 64-dim\n",
    "        self.fc = nn.Linear(5, 64)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, frames, gt_bboxes):\n",
    "        batch_size, seq_len, C, H, W = frames.shape\n",
    "        yolo_feats = torch.zeros(batch_size, seq_len, 64, device=self.device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                frame = frames[b, t]  # (3,640,640)\n",
    "                bbox_gt = gt_bboxes[b, t].tolist()  # [x1,y1,x2,y2]\n",
    "                # Run YOLO detection on this single frame\n",
    "                results = self.model.predict(source=frame.unsqueeze(0), imgsz=640, device=self.device, save=False, save_txt=False)\n",
    "                # results is a list with one element: a Results object\n",
    "                det = results[0].boxes  # det: Boxes object with .xyxy, .conf, .cls\n",
    "\n",
    "                best_iou = 0.0\n",
    "                best_feat = torch.zeros(5, device=self.device)  # placeholder [x1,y1,x2,y2,conf]\n",
    "\n",
    "                if det.shape[0] > 0:\n",
    "                    # det.xyxy is a tensor (num_dets, 4), det.conf is (num_dets,)\n",
    "                    boxes_xyxy = det.xyxy.cpu().numpy()  # shape (num_dets,4)\n",
    "                    confs = det.conf.cpu().numpy()       # shape (num_dets,)\n",
    "                    for i in range(boxes_xyxy.shape[0]):\n",
    "                        box = boxes_xyxy[i].tolist()\n",
    "                        conf = float(confs[i])\n",
    "                        iou = bbox_iou(box, bbox_gt)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_feat = torch.tensor([box[0], box[1], box[2], box[3], conf], device=self.device)\n",
    "\n",
    "                # Only accept if best_iou >= threshold; else leave feat=zero\n",
    "                if best_iou < self.iou_thresh:\n",
    "                    best_feat = torch.zeros(5, device=self.device)\n",
    "\n",
    "                # Pass through MLP → 64 dims\n",
    "                yolo_feats[b, t] = self.fc(best_feat)\n",
    "\n",
    "        return yolo_feats  # (batch, seq_len, 64)\n",
    "\n",
    "\n",
    "class SimpleCNNBackbone(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=224):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # After two poolings, size = img_size/4 × img_size/4\n",
    "        conv_output_size = (img_size // 4) * (img_size // 4) * 64\n",
    "        self.fc = nn.Linear(conv_output_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (batch*seq_len, 32, img_size/2, img_size/2)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (batch*seq_len, 64, img_size/4, img_size/4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)  # (batch*seq_len, 128)\n",
    "\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, D = x.size()\n",
    "        # expand cls token to batch\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, D)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1+S, D)\n",
    "        out = self.transformer(x)              # (B, 1+S, D)\n",
    "        return out[:, 0, :]  # return only the CLS token output  → (B, D)\n",
    "\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.intent_fc = nn.Linear(embed_dim, 1)\n",
    "        self.time_fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intent = torch.sigmoid(self.intent_fc(x))\n",
    "        ttc = self.time_fc(x)\n",
    "        return intent.squeeze(-1), ttc.squeeze(-1)\n",
    "\n",
    "\n",
    "class YOLOv12_IntentNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.device = cfg[\"device\"]\n",
    "        self.seq_len = cfg[\"seq_len\"]\n",
    "        self.img_size = cfg[\"img_size\"]\n",
    "\n",
    "        # 1. YOLOv12 backbone\n",
    "        self.yolo_backbone = YOLOv12Backbone(\n",
    "            weights_path=cfg[\"yolo_weights\"],\n",
    "            iou_thresh=cfg[\"iou_threshold_matching\"],\n",
    "            device=cfg[\"device\"]\n",
    "        )\n",
    "\n",
    "        # 2. Small CNN for cropped patches\n",
    "        self.cnn_backbone = SimpleCNNBackbone(img_size=self.img_size)\n",
    "\n",
    "        # 3. After concatenation (64 + 128 = 192), project to 256\n",
    "        self.linear_fuse = nn.Linear(64 + 128, 256)\n",
    "\n",
    "        # 4. Transformer for temporal modeling\n",
    "        self.temporal_model = TemporalTransformer(embed_dim=256, num_heads=4, num_layers=2)\n",
    "\n",
    "        # 5. Prediction head\n",
    "        self.pred_head = PredictionHead(embed_dim=256)\n",
    "\n",
    "    def forward(self, frames, gt_bboxes):\n",
    "        B, S, C, H, W = frames.shape\n",
    "\n",
    "        # 1. YOLO → (B, S, 64)\n",
    "        yolo_feats = self.yolo_backbone(frames, gt_bboxes)  # (B, S, 64)\n",
    "\n",
    "        # 2. Crop patches & get CNN features\n",
    "        patches = []\n",
    "        for b in range(B):\n",
    "            for t in range(S):\n",
    "                # We crop from the *original* 640×640 frame\n",
    "                img_np = (frames[b, t].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil = Image.fromarray(img_np)\n",
    "                x1, y1, x2, y2 = gt_bboxes[b, t].cpu().numpy().astype(int).tolist()\n",
    "                x1 = max(0, min(x1, W - 1))\n",
    "                y1 = max(0, min(y1, H - 1))\n",
    "                x2 = max(0, min(x2, W - 1))\n",
    "                y2 = max(0, min(y2, H - 1))\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    # If invalid box, crop a small center patch\n",
    "                    cx1, cy1 = W // 2 - self.img_size // 2, H // 2 - self.img_size // 2\n",
    "                    cx2, cy2 = cx1 + self.img_size, cy1 + self.img_size\n",
    "                    patch = pil.crop((cx1, cy1, cx2, cy2))\n",
    "                else:\n",
    "                    patch = pil.crop((x1, y1, x2, y2))\n",
    "                patch = patch.resize((self.img_size, self.img_size))\n",
    "                patch_tensor = transforms.ToTensor()(patch)\n",
    "                patch_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                    std=[0.229, 0.224, 0.225])(patch_tensor)\n",
    "                patches.append(patch_tensor)\n",
    "\n",
    "        patches_tensor = torch.stack(patches, dim=0)  # (B*S, 3, img_size, img_size)\n",
    "        cnn_feats = self.cnn_backbone(patches_tensor)  # (B*S, 128)\n",
    "        cnn_feats = cnn_feats.view(B, S, 128)           # (B, S, 128)\n",
    "\n",
    "        # 3. Concatenate YOLO feats + CNN feats → (B, S, 192)\n",
    "        combined = torch.cat([yolo_feats, cnn_feats], dim=2)  # (B, S, 64+128=192)\n",
    "\n",
    "        # Project to 256 dims\n",
    "        fused = self.linear_fuse(combined)  # (B, S, 256)\n",
    "\n",
    "        # 4. Transformer → (B, 256) [CLS token]\n",
    "        cls_out = self.temporal_model(fused)  # (B, 256)\n",
    "\n",
    "        # 5. Heads → (intent, ttc)\n",
    "        intent_pred, ttc_pred = self.pred_head(cls_out)  # each is (B,)\n",
    "        return intent_pred, ttc_pred\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_cls=1.0, lambda_reg=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_cls = lambda_cls\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, intent_pred, ttc_pred, label_cls, label_reg):\n",
    "        loss_cls = self.bce(intent_pred, label_cls)\n",
    "\n",
    "        # For regression, mask out samples where label_reg < 0\n",
    "        mask = (label_reg >= 0).float()\n",
    "        if mask.sum() > 0:\n",
    "            loss_reg = self.smooth_l1(\n",
    "                ttc_pred[mask == 1], \n",
    "                label_reg[mask == 1]\n",
    "            )\n",
    "        else:\n",
    "            loss_reg = torch.tensor(0.0, device=intent_pred.device)\n",
    "\n",
    "        total_loss = self.lambda_cls * loss_cls + self.lambda_reg * loss_reg\n",
    "        return total_loss, loss_cls.detach(), loss_reg.detach()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_cls = 0.0\n",
    "    running_reg = 0.0\n",
    "\n",
    "    for i, (frames, bboxes, label_cls, label_reg) in enumerate(dataloader):\n",
    "        # frames: (B, S, 3, 640,640)\n",
    "        # bboxes: (B, S, 4)\n",
    "        frames = frames.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "        label_cls = label_cls.to(device)\n",
    "        label_reg = label_reg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        intent_pred, ttc_pred = model(frames, bboxes)\n",
    "\n",
    "        loss, loss_cls, loss_reg = criterion(intent_pred, ttc_pred, label_cls, label_reg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_cls += loss_cls.item()\n",
    "        running_reg += loss_reg.item()\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "            avg_loss = running_loss / 20\n",
    "            avg_cls = running_cls / 20\n",
    "            avg_reg = running_reg / 20\n",
    "            print(f\"  Batch {i+1}/{len(dataloader)} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | BCL: {avg_cls:.4f} | S-L1: {avg_reg:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            running_cls = 0.0\n",
    "            running_reg = 0.0\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_cls = 0.0\n",
    "    val_reg = 0.0\n",
    "    all_intent_preds = []\n",
    "    all_intent_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (frames, bboxes, label_cls, label_reg) in enumerate(dataloader):\n",
    "            frames = frames.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "            label_cls = label_cls.to(device)\n",
    "            label_reg = label_reg.to(device)\n",
    "\n",
    "            intent_pred, ttc_pred = model(frames, bboxes)\n",
    "            loss, loss_cls, loss_reg = criterion(intent_pred, ttc_pred, label_cls, label_reg)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_cls += loss_cls.item()\n",
    "            val_reg += loss_reg.item()\n",
    "\n",
    "            # Collect for computing metrics\n",
    "            all_intent_preds.append(intent_pred.cpu())\n",
    "            all_intent_labels.append(label_cls.cpu())\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    avg_cls = val_cls / len(dataloader)\n",
    "    avg_reg = val_reg / len(dataloader)\n",
    "\n",
    "    # Compute simple accuracy for intent prediction\n",
    "    all_intents = torch.cat(all_intent_preds, dim=0)\n",
    "    all_labels = torch.cat(all_intent_labels, dim=0)\n",
    "    preds_binary = (all_intents >= 0.5).float()\n",
    "    acc = (preds_binary == all_labels).float().mean().item()\n",
    "\n",
    "    return avg_loss, avg_cls, avg_reg, acc\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# Main training loop\n",
    "def main_training(config):\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    # 1. Dataset & Dataloaders\n",
    "    full_dataset = PIESequenceDataset(\n",
    "        root_dir=config[\"dataset_root\"],\n",
    "        seq_len=config[\"seq_len\"],\n",
    "        img_size=config[\"img_size\"]\n",
    "    )\n",
    "    # Split 80/20\n",
    "    num_samples = len(full_dataset)\n",
    "    indices = list(range(num_samples))\n",
    "    random.shuffle(indices)\n",
    "    split = int(0.8 * num_samples)\n",
    "    train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        pin_memory=True if device != \"cpu\" else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        pin_memory=True if device != \"cpu\" else False\n",
    "    )\n",
    "\n",
    "    # 2. Model, Optimizer, Loss\n",
    "    model = YOLOv12_IntentNet(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-4)\n",
    "    criterion = CombinedLoss(\n",
    "        lambda_cls=config[\"lambda_cls\"],\n",
    "        lambda_reg=config[\"lambda_reg\"]\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{config['num_epochs']} =====\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_cls_l, val_reg_l, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Validation → Total Loss: {val_loss:.4f} | BCE: {val_cls_l:.4f} | S‐L1: {val_reg_l:.4f} | \"\n",
    "              f\"Intent Acc: {val_acc * 100:.2f}%\")\n",
    "\n",
    "        # Save checkpoint if improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            ckpt_path = os.path.join(config[\"save_dir\"], f\"best_model_epoch{epoch+1}.pt\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "            }, ckpt_path)\n",
    "            print(f\"  → Saved new best checkpoint: {ckpt_path}\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1} completed in {elapsed//60:.0f}m {elapsed%60:.0f}s\")\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    main_training(config)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def inference_example(config, checkpoint_path):\n",
    "    device = config[\"device\"]\n",
    "    # Rebuild dataset (same split)\n",
    "    full_dataset = PIESequenceDataset(\n",
    "        root_dir=config[\"dataset_root\"],\n",
    "        seq_len=config[\"seq_len\"],\n",
    "        img_size=config[\"img_size\"]\n",
    "    )\n",
    "    num_samples = len(full_dataset)\n",
    "    indices = list(range(num_samples))\n",
    "    random.shuffle(indices)\n",
    "    split = int(0.8 * num_samples)\n",
    "    val_idx = indices[split:]\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "\n",
    "    # Pick one random sample\n",
    "    sample_idx = random.choice(val_idx)\n",
    "    frames_tensor, bboxes_tensor, label_cls, label_reg = full_dataset[sample_idx]\n",
    "    # Add batch dimension\n",
    "    frames_batch = frames_tensor.unsqueeze(0).to(device)   # (1, S, 3, 640,640)\n",
    "    bboxes_batch = bboxes_tensor.unsqueeze(0).to(device)   # (1, S, 4)\n",
    "\n",
    "    # Build model & load checkpoint\n",
    "    model = YOLOv12_IntentNet(config).to(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Run forward\n",
    "    with torch.no_grad():\n",
    "        intent_p, ttc_p = model(frames_batch, bboxes_batch)\n",
    "        intent_p = intent_p.item()\n",
    "        ttc_p = ttc_p.item()\n",
    "\n",
    "    print(f\"Ground‐Truth Intent: {label_cls.item():.0f} | Predicted Intent: {intent_p:.3f}\")\n",
    "    print(f\"Ground‐Truth ttc (frames): {label_reg.item():.1f} | Predicted ttc (frames): {ttc_p:.3f}\")\n",
    "\n",
    "    # Visualize the S frames with GT bounding boxes\n",
    "    frames_np = (frames_tensor.permute(0, 2, 3, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "    fig, axs = plt.subplots(1, config[\"seq_len\"], figsize=(20, 3))\n",
    "    for t in range(config[\"seq_len\"]):\n",
    "        axs[t].imshow(frames_np[t])\n",
    "        x1, y1, x2, y2 = bboxes_tensor[t].numpy().tolist()\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
    "        axs[t].add_patch(rect)\n",
    "        axs[t].axis(\"off\")\n",
    "    plt.suptitle(f\"Pred Intent: {intent_p:.3f}, Pred TTC(frames): {ttc_p:.2f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14024233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
